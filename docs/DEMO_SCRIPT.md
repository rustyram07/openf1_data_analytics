# F1 Data Analytics - Live Demo Script

**Presenter Guide for Head of Data Platform Demo**

**Duration:** 15-20 minutes
**Audience:** Technical leadership
**Goal:** Demonstrate end-to-end data pipeline with real F1 data

---

## ğŸ¯ Demo Overview

**What We'll Show:**
1. Extract latest F1 session data from OpenF1 API (2 min)
2. Load raw data to Bronze tables (3 min)
3. Transform to Silver layer with dbt (5 min)
4. Create Gold analytics tables (3 min)
5. Query insights in Databricks (5 min)

**Architecture:**
```
OpenF1 API â†’ Bronze (Raw) â†’ Silver (Cleaned) â†’ Gold (Analytics)
```

---

## ğŸ“‹ Pre-Demo Setup (Do This Before Meeting)

### 1. Verify Environment
```bash
cd ~/Documents/openf1_data_analytics
source venv/bin/activate
python --version  # Should show Python 3.11+
```

### 2. Test Connection
```bash
cd openf1_api_data_extractor/src/utils
python testconnection.py
```

**Expected Output:**
```
Testing Databricks Connection...
âœ… Successfully connected to Databricks!
```

### 3. Clean Previous Data (Optional)
```sql
-- In Databricks SQL Editor (optional, for clean demo)
DROP TABLE IF EXISTS bronze_f1_data_analytics.bronze_sessions;
DROP TABLE IF EXISTS bronze_f1_data_analytics.bronze_drivers;
DROP TABLE IF EXISTS bronze_f1_data_analytics.bronze_laps;
DROP SCHEMA IF EXISTS stage_silver_clean CASCADE;
DROP SCHEMA IF EXISTS stage_gold_analytics CASCADE;
```

---

## ğŸ¬ Demo Script - Step by Step

### **Opening (1 min)**

**Say:**
> "Today I'll show you our production-ready F1 data analytics pipeline. We'll extract live data from Formula 1's official API, process it through a medallion architecture, and create analytics-ready tables - all in about 15 minutes."

**Show:**
- Architecture diagram (from README.md)
- Explain: Bronze (raw) â†’ Silver (cleaned) â†’ Gold (analytics)

---

### **STEP 1: Extract Latest F1 Session (2 min)**

**Say:**
> "First, let's extract the latest F1 session from the OpenF1 API. This API provides real-time telemetry, lap times, and driver data."

**Execute:**
```bash
cd ~/Documents/openf1_data_analytics/openf1_api_data_extractor/src/ingestion
python extract_sessions.py --num-sessions 1
```

**Expected Output:**
```
Fetching all available sessions...
Found 250 total sessions
Processing latest 1 session(s)

ğŸ“Š Session 1/1: 2024 Abu Dhabi Grand Prix - Race
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Session: Race (9165)
Location: Yas Marina Circuit, Abu Dhabi
Date: 2024-12-08

âœ… Extracted entities:
   Drivers: 20 drivers
   Laps: 1,147 laps
   Locations: 1,234,567 GPS points
   Sessions: 1 session

âœ… Saved to Databricks Volume:
   /Volumes/bronze_f1_data/landing/sessions_9165.json
   /Volumes/bronze_f1_data/landing/drivers_9165.json
   /Volumes/bronze_f1_data/landing/laps_9165.json
   /Volumes/bronze_f1_data/landing/locations_9165.json

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
All 1 sessions processed successfully!
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Explain:**
> "We just extracted data for the latest race. Notice we got 20 drivers, over 1,000 laps, and more than a million GPS telemetry points. All saved as JSON in our Databricks landing volume."

---

### **STEP 2: Load to Bronze Tables (3 min)**

**Say:**
> "Now let's load this raw data into Delta tables in our Bronze layer. Bronze stores data exactly as received - no transformations yet."

**Execute:**
```bash
cd ~/Documents/openf1_data_analytics/openf1_api_data_extractor/dlt_bronze_load
python run_batch_load.py
```

**Expected Output:**
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Bronze Layer Batch Load
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Data Directory: /Volumes/bronze_f1_data/landing/
Found 4 JSON files

  sessions: 1 files
  drivers: 1 files
  laps: 1 files
  locations: 1 files

Initializing Bronze Loader...

Loading bronze_sessions... âœ… Loaded 1 rows
Loading bronze_drivers... âœ… Loaded 20 rows
Loading bronze_laps... âœ… Loaded 1,147 rows
Loading bronze_locations... âœ… Loaded 1,234,567 rows

Verifying tables...
âœ… bronze_sessions: 1 rows
âœ… bronze_drivers: 20 rows
âœ… bronze_laps: 1,147 rows
âœ… bronze_locations: 1,234,567 rows

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Bronze tables loaded successfully!
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Show in Databricks UI:**
```sql
-- Quick check
SELECT * FROM bronze_f1_data_analytics.bronze_sessions LIMIT 1;
SELECT COUNT(*) FROM bronze_f1_data_analytics.bronze_laps;
```

**Explain:**
> "Bronze layer complete. We have raw data in Delta tables with ACID transactions, time travel, and schema enforcement. Notice we added an ingestion timestamp for data lineage."

---

### **STEP 3: Transform with dbt (Silver Layer) (5 min)**

**Say:**
> "Now we'll use dbt to transform this raw data into cleaned, business-ready tables in our Silver layer. dbt handles dependencies, testing, and documentation."

**Execute:**
```bash
cd ~/Documents/openf1_data_analytics/dbt_f1_data_analytics
dbt build --target stage --select silver_clean.*
```

**Expected Output:**
```
23:45:12  Running with dbt=1.10.13
23:45:13  Registered adapter: databricks=1.11.0
23:45:14  Found 10 models, 2 snapshots, 120 data tests, 4 sources, 829 macros
23:45:14
23:45:14  Concurrency: 4 threads (target='stage')
23:45:14

1 of 12 START sql incremental model stage_silver_clean.silver_sessions ... [RUN]
2 of 12 START sql incremental model stage_silver_clean.silver_drivers .... [RUN]
3 of 12 START sql incremental model stage_silver_clean.silver_laps ....... [RUN]
4 of 12 START sql incremental model stage_silver_clean.silver_locations .. [RUN]

1 of 12 OK created sql incremental model stage_silver_clean.silver_sessions [OK in 3.2s]
2 of 12 OK created sql incremental model stage_silver_clean.silver_drivers [OK in 3.5s]
3 of 12 OK created sql incremental model stage_silver_clean.silver_laps [OK in 4.1s]
4 of 12 OK created sql incremental model stage_silver_clean.silver_locations [OK in 12.3s]

5 of 12 START test not_null_silver_sessions_session_key ................ [RUN]
6 of 12 START test not_null_silver_drivers_driver_number ............... [RUN]
...
12 of 12 PASS test unique_silver_laps_composite_key .................... [PASS in 2.1s]

Done. PASS=12 WARN=0 ERROR=0 SKIP=0 TOTAL=12
```

**Show in Databricks:**
```sql
-- Silver sessions with business logic
SELECT
    session_name,
    location,
    session_category,
    session_duration_minutes,
    year
FROM stage_silver_clean.silver_sessions;

-- Cleaned driver data
SELECT
    full_name,
    team_name,
    team_category,
    driver_number
FROM stage_silver_clean.silver_drivers
ORDER BY driver_number;
```

**Explain:**
> "Silver layer adds business value: session categories, team standardization, data quality tests. Notice dbt ran 12 tests automatically - all passed. This data is now clean and trusted."

---

### **STEP 4: Build Gold Analytics Layer (3 min)**

**Say:**
> "Finally, let's build our Gold layer - dimensional models and analytics aggregations optimized for BI tools and data science."

**Execute:**
```bash
dbt build --target stage --select gold_analytics.*
```

**Expected Output:**
```
1 of 8 START sql table model stage_gold_analytics.dim_sessions ........ [RUN]
2 of 8 START sql table model stage_gold_analytics.dim_drivers ......... [RUN]
3 of 8 START sql table model stage_gold_analytics.fact_laps ........... [RUN]

1 of 8 OK created sql table model stage_gold_analytics.dim_sessions ... [OK in 2.1s]
2 of 8 OK created sql table model stage_gold_analytics.dim_drivers .... [OK in 3.4s]
3 of 8 OK created sql table model stage_gold_analytics.fact_laps ...... [OK in 5.6s]

4 of 8 START sql view model stage_gold_analytics.session_leaderboard .. [RUN]
5 of 8 START sql view model stage_gold_analytics.driver_session_summary [RUN]
6 of 8 START sql view model stage_gold_analytics.team_performance ..... [RUN]

4 of 8 OK created sql view model stage_gold_analytics.session_leaderboard [OK in 1.2s]
5 of 8 OK created sql view model stage_gold_analytics.driver_session_summary [OK in 1.5s]
6 of 8 OK created sql view model stage_gold_analytics.team_performance [OK in 1.3s]

Done. PASS=8 WARN=0 ERROR=0 SKIP=0 TOTAL=8
```

**Explain:**
> "Gold layer complete. We have star schema dimensions, fact tables, and pre-aggregated marts. These are optimized for dashboards and analysis."

---

### **STEP 5: Show Analytics Insights (5 min)**

**Say:**
> "Let's explore some insights from the data we just processed."

#### Query 1: Race Results (Fastest Laps)
```sql
SELECT
    position,
    driver_name,
    team_name,
    fastest_lap_time,
    ROUND(delta_to_fastest, 3) as seconds_behind,
    ROUND(gap_percentage, 2) as gap_pct
FROM stage_gold_analytics.session_leaderboard
WHERE is_top_3 = TRUE
ORDER BY position
LIMIT 3;
```

**Expected Result:**
```
position | driver_name      | team_name        | fastest_lap_time | seconds_behind | gap_pct
---------|------------------|------------------|------------------|----------------|--------
1        | Max VERSTAPPEN   | Red Bull Racing  | 84.543          | 0.000          | 0.00
2        | Charles LECLERC  | Ferrari          | 84.721          | 0.178          | 0.21
3        | Lando NORRIS     | McLaren          | 84.856          | 0.313          | 0.37
```

**Say:**
> "Top 3 finishers with their fastest laps and gaps. Verstappen won by 0.2 seconds."

---

#### Query 2: Driver Performance
```sql
SELECT
    driver_name,
    team_name,
    total_laps,
    valid_laps,
    avg_lap_time,
    fastest_lap,
    ROUND(consistency_coefficient, 3) as consistency
FROM stage_gold_analytics.driver_session_summary
ORDER BY fastest_lap
LIMIT 5;
```

**Expected Result:**
```
driver_name      | team_name       | total_laps | valid_laps | avg_lap_time | fastest_lap | consistency
-----------------|-----------------|------------|------------|--------------|-------------|------------
Max VERSTAPPEN   | Red Bull Racing | 58         | 56         | 88.234       | 84.543      | 0.042
Charles LECLERC  | Ferrari         | 58         | 55         | 88.567       | 84.721      | 0.045
Lando NORRIS     | McLaren         | 58         | 57         | 88.891       | 84.856      | 0.039
```

**Say:**
> "Driver performance metrics. Lower consistency coefficient means more consistent lap times - Norris was most consistent."

---

#### Query 3: Team Performance
```sql
SELECT
    team_name,
    team_fastest_lap,
    drivers_count,
    avg_driver_fastest_lap,
    team_rank
FROM stage_gold_analytics.team_performance
ORDER BY team_rank
LIMIT 5;
```

**Expected Result:**
```
team_name        | team_fastest_lap | drivers_count | avg_driver_fastest_lap | team_rank
-----------------|------------------|---------------|------------------------|----------
Red Bull Racing  | 84.543          | 2             | 84.892                 | 1
Ferrari          | 84.721          | 2             | 85.234                 | 2
McLaren          | 84.856          | 2             | 85.445                 | 3
```

**Say:**
> "Team standings based on fastest laps. Red Bull dominated with the quickest lap overall."

---

#### Query 4: Data Lineage
```sql
-- Show the data pipeline
SELECT
    'Bronze' as layer,
    'bronze_laps' as table_name,
    COUNT(*) as row_count,
    MAX(_ingestion_timestamp) as last_updated
FROM bronze_f1_data_analytics.bronze_laps

UNION ALL

SELECT
    'Silver' as layer,
    'silver_laps' as table_name,
    COUNT(*) as row_count,
    MAX(updated_at) as last_updated
FROM stage_silver_clean.silver_laps

UNION ALL

SELECT
    'Gold' as layer,
    'fact_laps' as table_name,
    COUNT(*) as row_count,
    MAX(updated_at) as last_updated
FROM stage_gold_analytics.fact_laps

ORDER BY layer;
```

**Say:**
> "Complete data lineage. We can trace data from API to analytics, with timestamps showing when each layer was updated."

---

### **STEP 6: Show Automation (2 min)**

**Say:**
> "This entire process is automated with GitHub Actions."

**Show Workflows:**

Open GitHub Actions page:
```
https://github.com/rustyram07/openf1_data_analytics/actions
```

**Point out:**
1. **Data Ingestion Workflow**
   - Runs every 2 hours during race weekends
   - Extracts latest session automatically
   - Loads to Bronze
   - Triggers dbt

2. **dbt CI/CD Workflow**
   - Runs on every PR
   - Tests against stage environment
   - Validates data quality
   - Deploys to production on merge

**Say:**
> "We have two workflows: one for data ingestion, one for transformations. Both run automatically - during race weekends, fresh data flows through the pipeline every 2 hours."

---

### **STEP 7: Show dbt Documentation (2 min)**

**Execute:**
```bash
cd ~/Documents/openf1_data_analytics/dbt_f1_data_analytics
dbt docs generate
dbt docs serve
```

**Browser opens to:** http://localhost:8080

**Show:**
1. **Lineage Graph** - Visual data flow
2. **Model Documentation** - Column descriptions
3. **Tests** - Data quality checks
4. **Sources** - Bronze table references

**Say:**
> "Self-documenting pipeline. Every model has descriptions, lineage, and automated tests. Data team can understand the pipeline without asking questions."

---

## ğŸ¯ Closing (2 min)

### Key Takeaways

**Say:**
> "In 15 minutes, we demonstrated:
>
> 1. âœ… **Real-time data ingestion** from OpenF1 API
> 2. âœ… **Medallion architecture** - Bronze, Silver, Gold
> 3. âœ… **Data quality** - 120+ automated tests
> 4. âœ… **CI/CD automation** - GitHub Actions
> 5. âœ… **Self-documentation** - dbt docs
> 6. âœ… **Production-ready** - ACID transactions, time travel, lineage
>
> This pattern works for any data source - not just F1. The medallion architecture, dbt transformations, and automated testing are industry best practices we can apply across all data pipelines."

---

## ğŸ“Š Business Value Highlights

**Mention:**
- **Time to insights:** 15 minutes from API to analytics
- **Data quality:** Automated testing catches issues before production
- **Cost efficiency:** Incremental processing, not full reloads
- **Scalability:** Handles millions of GPS points per session
- **Maintainability:** Self-documenting, version controlled
- **Reliability:** ACID transactions, schema enforcement

---

## ğŸ› ï¸ Technical Highlights (If Asked)

- **Delta Lake:** ACID transactions, time travel, schema evolution
- **dbt:** Version controlled transformations, dependency management
- **GitHub Actions:** CI/CD, automated testing
- **Python:** Data extraction, orchestration
- **SQL:** Analytics queries, transformations
- **Databricks:** Unified analytics platform

---

## ğŸ’¡ Potential Questions & Answers

**Q: How much does this cost to run?**
> A: Minimal. Databricks scales to zero when not in use. Data ingestion runs 2 hours during race weekends (~$5-10/month). Storage is pennies for millions of records.

**Q: Can we apply this to our data sources?**
> A: Absolutely. Replace OpenF1 API with your source (REST API, database, files). Same Bronzeâ†’Silverâ†’Gold pattern applies.

**Q: How do you handle schema changes?**
> A: Delta Lake supports schema evolution. dbt tests catch breaking changes. We version control all transformations for rollback capability.

**Q: What about data governance?**
> A: Unity Catalog provides lineage, access control, and audit logs. dbt docs show column-level lineage. All changes are tracked in git.

**Q: Performance at scale?**
> A: Databricks auto-scales. We tested with full 2024 season (25 races Ã— millions of GPS points). Queries still sub-second with proper partitioning.

**Q: How long to set up for new data source?**
> A: 1-2 days for extraction logic. Bronzeâ†’Silverâ†’Gold framework is reusable. dbt tests and docs come free.

---

## ğŸ¬ Demo Tips

### Before Demo:
- [ ] Test entire flow once
- [ ] Clean previous data for fresh demo
- [ ] Have Databricks SQL Editor open
- [ ] Have GitHub Actions page bookmarked
- [ ] Terminal font size readable
- [ ] Check internet connection

### During Demo:
- **Speak clearly** - explain what's happening
- **Pause for questions** - don't rush
- **Show, don't just tell** - actual data, not slides
- **Be ready to improvise** - queries might return different results
- **Have backup plan** - if API fails, use test data

### After Demo:
- Share documentation links
- Offer follow-up session
- Send GitHub repo link

---

## ğŸ“ Quick Commands Reference

```bash
# Navigate to project
cd ~/Documents/openf1_data_analytics
source venv/bin/activate

# Extract data
cd openf1_api_data_extractor/src/ingestion
python extract_sessions.py --num-sessions 1

# Load bronze
cd ../../dlt_bronze_load
python run_batch_load.py

# Transform with dbt
cd ../../../dbt_f1_data_analytics
dbt build --target stage

# View docs
dbt docs generate && dbt docs serve

# Check workflows
gh run list
```

---

**Good luck with your demo! ğŸï¸ğŸ’¨**
